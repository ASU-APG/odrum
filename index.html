<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="author" content="Tejas Gokhale">
  <title>O-DRUM CVPR 2022</title>
  <link rel="stylesheet" type="text/css" href="css/simpleGridTemplate.css">
  <link rel="icon" type="image/png" href="images/drum.png">
  <link href="https://use.fontawesome.com/releases/v5.0.4/css/all.css" rel="stylesheet">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="icon" href="./favicon.png" type="image/png" />
  <style>
    body {
      overflow-y: scroll;
    }
    .markdown-body h1 {
      display: flex;
    }
    .markdown-body form {
      margin-left: 10px;
    }
    .markdown-body input {
      margin: 0;
      padding: 0 10px;
      border: 1px solid #eaecef;
      border-radius: 3px;
      width: 100%;
      flex: 1 1;
    }
    table, tr, td {
      border: none;
    }
    table.fixed {table-layout:fixed; width:90px;}/*Setting the table width is important!*/
    table.fixed td {overflow:hidden;}/*Hide text outside the cell.*/
    table.fixed td:nth-of-type(1) {width:250px;}/*Setting the width of column 1.*/
    table.fixed td:nth-of-type(2) {width:250px;}/*Setting the width of column 2.*/
    table.fixed td:nth-of-type(3) {width:250px;}/*Setting the width of column 3.*/
    table.fixed td:nth-of-type(4) {width:250px;}/*Setting the width of column 4.*/
    table.fixed td:nth-of-type(5) {width:250px;}/*Setting the width of column 5.*/
    .centered-and-cropped { object-fit: cover }
  </style>
</head>

<body>
  <div class="thumbnail" align="center">
    <table class="title">
      <tr class="title">
        <td class="title" style="text-align: center;"><img src="images/drum.png"  height=200px></td>
        <td class="title">
          <h1 style="text-align:center; font-weight:600;">O-DRUM @ CVPR 2023</h1>
          <h1 style="text-align:center; font-weight:400;">Workshop on Open-Domain Reasoning Under Multi-Modal Settings</h1>
          <h3 style="text-align:center;"> coming soon !!!</h3>
          <h3 style="text-align:center;"> Montreal</h3>
          <br/>
          <h2 style="text-align:center;">ODRUM 2022 Archive: <a href="./archive_2022.html">[Webpage]</a> <a href="https://youtu.be/vxizyhlTRLU"> YouTube </a></h2>

        </td>
      </tr>
    </table>
    <hr>
    <p>
      In the past decade, there has been a paradigm shift in computer vision research -- the connection between language and vision is now an integral part of various of the philosophy of computer vision. 
      Vision+Language models have not only impacted multimodal tasks but also a wide variety of tasks in fundamental computer vision and computer graphics.
      The link between vision and language is much more complex than simple image--text alignment -- language is used for reasoning beyond the visible (for example, physical reasoning, spatial reasoning, commonsense reasoning, and embodied reasoning).
      This is the challenging frontier that the computer vision community is gradually warming up to -- Open-Domain Reasoning in Multi-Modal Settings (ODRUM 2023) is an ideally-suited platform for discussions on multimodal (vision+language) topics with special emphasis on reasoning capabilities.
    </p>
    <p>
      The aim of ODRUM 2023 is to address the emerging topic of visual reasoning using multiple modalities (such as text, images, videos, audio, etc.).
      The workshop will feature invited talks by experts at the forefront of research in the realm of reasoning such as: embodied AI and embodied navigation, learning via interaction and collaboration with humans, buildng large V+L that can perform multiple tasks, visual grounding and reasoning, and the use of language to instruct robots.
      At the end of the workshop, participants and speakers will converge for a panel discussion -- the panel will discuss the importance of reasoning (a core AI topic that has a rich and long history since the 1950s) to computer vision, and relevance to recent progress in visual reasoning.
      The panel will discuss trends and challenges in open-domain reasoning, from different perspectives of NLP, vision, machine learning, and robotics researchers.
    </p>

    <br/><hr/><br/>

    <h1>Confirmed Speakers</h1>
    <i>Schedule coming soon. </i>

    <table cellspacing="0" cellpadding="0" class="fixed">
      <tr>
        <td>
          <a href="https://www.cs.utexas.edu/users/grauman/"><center>
            <img class="centered-and-cropped" height="250px" style="border-radius:50%" src="https://www.cs.utexas.edu/users/grauman/grauman.jpg" >
            <br><big><b>Kristen Grauman</b></big>
            <br/>Professor
            <br/>University of Texas at Austin
          </center></a>
        </td>
        <td>
          <a href="https://jiajunwu.com/"><center>
            <img src="https://jiajunwu.com/images/Jiajun_Wu.jpg" height="250px" style="border-radius:50%">
            <br><big><b>Jiajun Wu</b></big>
            <br/>Assistant Professor
            <br/>Stanford University
          </center></a>
        </td>
        <td>
          <a href="https://www.alanesuhr.com/"><center>
            <img src="https://www.alanesuhr.com/photos/photo29-small.png" height="250px" style="border-radius:50%">
            <br><big><b>Alane Suhr</b></big>
            <br/>Young Investigator
            <br/>Allen Institute for AI
          </center></a>
        </td>
        <td>
          <a href="https://www.jbalayrac.com/"><center>
            <img class="centered-and-cropped" src="https://www.jbalayrac.com/photos/id3.jpg" height="250px" style="border-radius:50%">
            <br><big><b>Jean_Baptiste Alayrac</b></big>
            <br/>Research Scientist
            <br/>Deepmind
          </center></a>
        </td>
        <td>
          <a href="https://angelxuanchang.github.io/"><center>
            <img src="https://angelxuanchang.github.io/files/angel.jpg" height="250px" style="border-radius:50%">
            <br><big><b>Angel Xuan Chang</b></big>
            <br/>Assistant Professor
            <br/>Simon Fraser University
          </center></a>
        </td>
      </tr>
    </table>

    <br/><hr/><br/>



    <h1>Call for Papers</h1>
    We invite submissions related to the broad topic area of multi-modal understanding, reasoning and comprehension, including but not limited to following topics:
    <ul>
      <li> Visual Reasoning using multiple modalities such as images, videos, audio, and text </li>
      <li> Visual Grounding with natural language </li>
      <li> Embodied perception, such as vision--language navigation </li>
      <li> Use of natural language, instructions, or other vision--language supervision in robotics </li>
      <li> Collaborative and multimodal learning, with human interaction and/or feedback </li>
      <li> Multimodal information retrieval </li>
      <li> Methods of retrieval that facilitate additional tasks, such as image and video captioning, visual grounding, VQA, image generation, and graphics, among others
      </li>
      <li> Utilization of Retrieval in unsupervised/few-shot/zero-shot learning for data augmentation and generation. </li>
      <li> New datasets or task designs for open-domain reasoning, use of knowledge bases in multimodal reasoning, or any other multimodal tasks </li>
      <li> Modification and enhancement of existing multi-modal comprehension tasks, including VQA, VQA with Knowledge (e.g., OK-VQA, Web-QA, etc).</li>
      <li> Design of new evaluation metrics for multimodal tasks </li>
      <li> Analysis, commentary, or position pieces on existing evaluation metrics in multimodal tasks </li>
      <li> Image and video analytics </li>
      <li> Efficient and/or robust representation learning </li>
      <li> Reliability and Robustness issues in multimodal learning and reasoning </li>
      <li> The role of external knowledge in multimodal reasoning </li>
      <li> Continual / Lifelong / Online multimodal learning </li>
    </ul>
    <p>
      We encourage two types of submissions:
      <ul>
        <li> Extended abstracts (four pages plus an unlimited number of references) </li>
        <li> Long papers (8 pages maximum with unlimited references) </li>
      </ul>
    </p>
    <p>
      All submitted materials must be anonymized and formatted according to the <a href="https://cvpr2023.thecvf.com/Conferences/2023/AuthorGuidelines"> CVPR 2023 author guidelines and template</a>. Accepted papers will be presented as posters at the workshop, where attendees, invited speakers, and organizers will engage in discussion. We intend to highlight three best papers during the workshop session with spotlight presentations. We will give authors of all accepted papers an option to opt-in or opt-out of CVPR proceedings.
    </p>

    <br/>
    <h3>Important Dates:</h3>
      <table>
        <tr>
          <td>&diams; <i>Submission Deadline:</i></td><td>March 20, 2023, 23:59 PDT</td>
        </tr>
        <tr>
          <td>&diams; <i>Notification of Decision:</i></td><td>March 31, 2023</td>
        </tr>
        <tr>
          <td>&diams; <i>Camera Ready Deadline:</i></td><td>April 08, 2022 , 12:00 PDT</td>
        </tr>
        <tr>
          <td>Submission website (CMT):</td>
          <td> Coming Soon </td>
          <!-- <td><a href="https://cmt3.research.microsoft.com/ODRUM2022">https://cmt3.research.microsoft.com/ODRUM2022</a> --></td>
        </tr>
      </table>
      <br/>


    <br/><hr/><br/>
   

    <h1> Organizers </h1>
    <div class="container">
      <div class="hosts">
        <table class="person" align="center">
          <tr>
            
            <td class="person" style="text-align: center;">
              <a href="https://tejasgokhale.com">
                <img class="centered-and-cropped" height="160px" style="border-radius:50%" src="https://www.tejasgokhale.com/images/tg_brickyard.jpg">
              <br><b>Tejas Gokhale</b></a>
              <br>ASU
            </td>
            <td class="person" style="text-align: center;">
              <a href="https://luomancs.github.io/">
                <img class="centered-and-cropped" height="160px" style="border-radius:50%" src="images/man.jpg">
              <br><b>Man Luo</b></a>
              <br>ASU
            </td>
            <td class="person" style="text-align: center;">
              <a href="https://yezhouyang.engineering.asu.edu/">
                <img class="centered-and-cropped" height="160px" style="border-radius:50%" src="images/yezhou.png">
              <br><b>Yezhou Yang</b></a>
              <br>ASU
            </td> 
            <td class="person" style="text-align: center;">
              <a href="https://www.public.asu.edu/~cbaral/">
                <img class="centered-and-cropped" height="160px" style="border-radius:50%" src="images/cbaral.png">
                <br><b>Chitta Baral</b></a>
                <br>ASU<br>
            </td>
            <td class="person" style="text-align: center;">
              <a href="http://kennethmarino.com/">
                <img class="centered-and-cropped" height="160px" style="border-radius:50%" src="images/kenny.PNG">
              <br><b>Kenneth Marino</b></a>
              <br>Deepmind
            </td>
             <td class="person" style="text-align: center;">
              <a href="https://www.public.asu.edu/~zfang29/">
                <img class="centered-and-cropped" height="160px" style="border-radius:50%" src="images/jacob.jpg">
              <br><b>Zhiyuan Fang</b></a>
              <br>Amazon Alexa AI
            </td>
            <td class="person" style="text-align: center;">
              <a href="https://pratyay-banerjee.github.io/">
                <img class="centered-and-cropped" height="160px" style="border-radius:50%" src="images/pratyay.jpg">
              <br><b>Pratyay Banerjee</b></a>
              <br>Amazon Alexa AI
            </td>
          </tr>                     
        </table>

      </div>
      <hr>
      Please contact Man Luo (<a href="mailto: mluo26@asu.edu">mluo26@asu.edu</a>) or Tejas Gokhale (<a href="mailto: tgokhale@asu.edu">tgokhale@asu.edu</a>) for additional details
      <hr>
      <i> The workshop is supported by US National Science Foundation grants 1816039, 2132724 as part of Research, Education, and Outreach activities. </i>
      <hr>
      <p align='right'><i>Website maintained by Tejas Gokhale </i> </p>

  </div>
<!-- </div> -->
</body>
</html>
