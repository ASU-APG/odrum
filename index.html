<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="author" content="Tejas Gokhale">
  <meta property="og:image" content="images/drum.png" />
  <title>O-DRUM CVPR 2023</title>
  <link rel="stylesheet" type="text/css" href="css/simpleGridTemplate.css">
  <link rel="icon" type="image/png" href="images/drum.png">
  <link href="https://use.fontawesome.com/releases/v5.0.4/css/all.css" rel="stylesheet">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="icon" href="./favicon.png" type="image/png" />

  <style>
    body {
      overflow-y: scroll;
    }
    .markdown-body h1 {
      display: flex;
    }
    .markdown-body form {
      margin-left: 10px;
    }
    .markdown-body input {
      margin: 0;
      padding: 0 10px;
      border: 1px solid #eaecef;
      border-radius: 3px;
      width: 100%;
      flex: 1 1;
    }
    table, tr, td {
      border: none;
    }
    table.fixed {table-layout:fixed; width:90px;}/*Setting the table width is important!*/
    table.fixed td {overflow:hidden;}/*Hide text outside the cell.*/
    table.fixed td:nth-of-type(1) {width:250px;}/*Setting the width of column 1.*/
    table.fixed td:nth-of-type(2) {width:250px;}/*Setting the width of column 2.*/
    table.fixed td:nth-of-type(3) {width:250px;}/*Setting the width of column 3.*/
    table.fixed td:nth-of-type(4) {width:250px;}/*Setting the width of column 4.*/
    table.fixed td:nth-of-type(5) {width:250px;}/*Setting the width of column 5.*/
    .centered-and-cropped { object-fit: cover }
  </style>
</head>

<body>
  <div class="thumbnail" align="center">
    <table class="title">
      <tr class="title">
        <td class="title" style="text-align: center;"><img src="images/drum.png"  height=200px></td>
        <td class="title">
          <h1 style="text-align:center; font-weight:600;">O-DRUM @ CVPR 2023</h1>
          <h1 style="text-align:center; font-weight:400;">Workshop on Open-Domain Reasoning Under Multi-Modal Settings</h1>
          <h3 style="text-align:center;"> June 19, 2023 <br/> 8:30 AM -- 5:00 PM PDT <br/> Vancouver</h3>
          <br/>
          <h2 style="text-align:center;">ODRUM 2022 Archive: <a href="./archive_2022.html">[Webpage]</a> <a href="https://youtu.be/vxizyhlTRLU"> YouTube </a></h2>
        </td>
      </tr>
    </table>
    <hr>
    <p>
      AI has undergone a paradigm shift in the past decade -- the connection between vision and language (V+L) is now an integral part of AI, with deep impact beyond vision and NLP -- robotics, graphics, cybersecurity, and HCI are utilizing V+L tools and there are direct industrial implications for software, arts, and media. The link between vision and language is much more complex than simple image--text alignment â€“ the use of language for reasoning beyond the visible (for example, physical reasoning, spatial reasoning, commonsense reasoning, and embodied reasoning) is being pursued. Open-Domain Reasoning in Multi-Modal Settings (ODRUM 2023) provides a platform for discussions on multimodal (vision+language) topics with special emphasis on reasoning capabilities.
    </p>
    <p>
      The aim of ODRUM 2023 is to address the emerging topic of visual reasoning using multiple modalities (such as text, images, videos, audio, etc.). The workshop will feature invited talks by experts in the realm of reasoning such as: embodied AI, navigation, learning via interaction and collaboration with humans, building large V+L that can perform multiple tasks, visual grounding, and the use of language to instruct robots. Participants and speakers will converge for a panel discussion to discuss the importance of reasoning (a core AI topic that has a rich and long history since the 1950s) to computer vision, relevance to recent progress in visual reasoning, discuss trends and challenges in open-domain reasoning, from different perspectives of NLP, vision, machine learning, and robotics researchers.

    </p>

    <br/><hr/><br/>

    <h1>Confirmed Speakers</h1>

    <table cellspacing="0" cellpadding="0" class="fixed">
      <tr>
        <td>
          <a href="https://www.cs.utexas.edu/users/grauman/"><center>
            <img class="centered-and-cropped" height="200px" style="border-radius:30%" src="images/kristen.jpg" >
            <br><big><b>Kristen Grauman</b></big>
            <br/>Professor
            <br/>University of Texas at Austin
          </center></a>
        </td>
        <td>
          <a href="https://jiajunwu.com/"><center>
            <img src="https://jiajunwu.com/images/Jiajun_Wu.jpg" height="200px" style="border-radius:30%">
            <br><big><b>Jiajun Wu</b></big>
            <br/>Assistant Professor
            <br/>Stanford University
          </center></a>
        </td>
        <td>
          <a href="https://www.alanesuhr.com/"><center>
            <img src="images/alane.jpg" height="200px" style="border-radius:30%">
            <br><big><b>Alane Suhr</b></big>
            <br/>Young Investigator
            <br/>Allen Institute for AI
          </center></a>
        </td>
        <td>
          <a href="https://www.robots.ox.ac.uk/~karel/"><center>
            <img class="centered-and-cropped" src="karel.jpeg" height="200px" style="border-radius:30%">
            <br><big><b>Karel Lenc</b></big>
            <br/>Research Scientist
            <br/>Deepmind
          </center></a>
        </td>
        <td>
          <a href="https://angelxuanchang.github.io/"><center>
            <img src="images/angel.jpg" height="200px" style="border-radius:30%">
            <br><big><b>Angel Xuan Chang</b></big>
            <br/>Assistant Professor
            <br/>Simon Fraser University
          </center></a>
        </td>
      </tr>
    </table>


    <h2>Tentative Schedule</h2> 
    <table>
      <tr><td>08:30 -- 08:45</td><td>Welcome and Introduction</td></tr>
      <tr><td>08:45 -- 09:35</td><td>Invited Talk 1</td></tr>
      <tr><td>09:35 -- 10:00</td><td>Spotlight Talks</td></tr>
      <tr><td>10:00 -- 10:40</td><td>Poster Session + Coffee Break</td></tr>
      <tr><td>10:40 -- 11:30</td><td>Invited Talk 2</td></tr>
      <tr><td>11:30 -- 12:20</td><td>Invited Talk 3</td></tr>
      <tr><td>12:20 -- 13:20</td><td>Lunch</td></tr>
      <tr><td>13:20 -- 14:10</td><td>Invited Talk 4</td></tr>
      <tr><td>14:10 -- 15:00</td><td>Invited Talk 5</td></tr>
      <tr><td>15:00 -- 15:45</td><td>Poster Session 2 + Coffee Break</td></tr>
      <tr><td>15:45 -- 17:00</td><td>Panel Discussion + Concluding Remarks</td></tr>
    </table>
    <br/><hr/><br/>



    <h1>Call for Papers</h1>
    We invite submissions related to the broad topic area of multi-modal understanding, reasoning and comprehension, including but not limited to following topics:
    <ul>
      <li> Visual Reasoning using multiple modalities such as images, videos, audio, and text </li>
      <li> Visual Grounding with natural language </li>
      <li> Embodied perception, such as vision--language navigation </li>
      <li> Use of natural language, instructions, or other vision--language supervision in robotics </li>
      <li> Collaborative and multimodal learning, with human interaction and/or feedback </li>
      <li> Multimodal information retrieval </li>
      <li> Methods of retrieval that facilitate additional tasks, such as image and video captioning, visual grounding, VQA, image generation, and graphics, among others
      </li>
      <li> Utilization of Retrieval in unsupervised/few-shot/zero-shot learning for data augmentation and generation. </li>
      <li> New datasets or task designs for open-domain reasoning, use of knowledge bases in multimodal reasoning, or any other multimodal tasks </li>
      <li> Modification and enhancement of existing multi-modal comprehension tasks, including VQA, VQA with Knowledge (e.g., OK-VQA, Web-QA, etc).</li>
      <li> Design of new evaluation metrics for multimodal tasks </li>
      <li> Analysis, commentary, or position pieces on existing evaluation metrics in multimodal tasks </li>
      <li> Image and video analytics </li>
      <li> Efficient and/or robust representation learning </li>
      <li> Reliability and Robustness issues in multimodal learning and reasoning </li>
      <li> The role of external knowledge in multimodal reasoning </li>
      <li> Continual / Lifelong / Online multimodal learning </li>
    </ul>
    <p>
      We encourage two types of submissions:
      <ul>
        <li> Extended abstracts (four pages plus an unlimited number of references) </li>
        <li> Long papers (8 pages maximum with unlimited references) </li>
      </ul>
    </p>
    <p>
      All submitted materials must be anonymized and formatted according to the <a href="https://cvpr2023.thecvf.com/Conferences/2023/AuthorGuidelines"> CVPR 2023 author guidelines and template</a>. Accepted papers will be presented as posters at the workshop, where attendees, invited speakers, and organizers will engage in discussion. We intend to highlight three best papers during the workshop session with spotlight presentations. We will give authors of all accepted papers an option to opt-in or opt-out of CVPR proceedings.
    </p>

    <br/>
    <h3>Important Dates:</h3>
      <table border="1" cellpadding="15">
        <tr>
          <td>&diams; <i>Submission Deadline:</i></td><td>March 24, 2023, 23:59 PDT</td>
        </tr>
        <tr>
          <td>&diams; <i>Notification of Decision:</i></td><td>March 31, 2023</td>
        </tr>
        <tr>
          <td>&diams; <i>Camera Ready Deadline:</i></td><td>April 08, 2022 , 12:00 PDT</td>
        </tr>
        <tr>
          <td>&diams; <i>Submission website (CMT):</i></td>
          <td> <a href="https://cmt3.research.microsoft.com/ODRUM2023/">https://cmt3.research.microsoft.com/ODRUM2023/</a> </td>
          <!-- <td><a href="https://cmt3.research.microsoft.com/ODRUM2022">https://cmt3.research.microsoft.com/ODRUM2022</a> --></td>
        </tr>
      </table>
      <br/>


    <br/><hr/><br/>
   

    <h1> Organizers </h1>
    <div class="container">
      <div class="hosts">
        <table class="person" align="center">
          <tr>
            
            <td class="person" style="text-align: center;">
              <a href="https://tejasgokhale.com">
                <img class="centered-and-cropped" height="160px" style="border-radius:30%" src="https://www.tejasgokhale.com/images/tejas/tg_hawaii_square.png">
              <br><b>Tejas Gokhale</b></a>
              <br>Assistant Professor
              <br>UMBC
            </td>
            <td class="person" style="text-align: center;">
              <a href="https://luomancs.github.io/">
                <img class="centered-and-cropped" height="160px" style="border-radius:30%" src="images/man.jpg">
              <br><b>Man Luo</b></a>
              <br/>Postdoctoral Researcher
              <br/>Mayo Clinic
            </td>
            <td class="person" style="text-align: center;">
              <a href="https://yezhouyang.engineering.asu.edu/">
                <img class="centered-and-cropped" height="160px" style="border-radius:30%" src="images/yezhou.png">
              <br><b>Yezhou Yang</b></a>
              <br>Associate Professor
              <br>ASU
            </td> 
            <td class="person" style="text-align: center;">
              <a href="https://www.public.asu.edu/~cbaral/">
                <img class="centered-and-cropped" height="160px" style="border-radius:30%" src="images/cbaral.png">
                <br><b>Chitta Baral</b></a>
                <br>Professor
                <br>ASU<br>
            </td>
            <td class="person" style="text-align: center;">
              <a href="http://kennethmarino.com/">
                <img class="centered-and-cropped" height="160px" style="border-radius:30%" src="images/kenny.PNG">
              <br><b>Kenneth Marino</b></a>
              <br>Research Scientist
              <br>Deepmind
            </td>
             <td class="person" style="text-align: center;">
              <a href="https://www.public.asu.edu/~zfang29/">
                <img class="centered-and-cropped" height="160px" style="border-radius:30%" src="images/jacob.jpg">
              <br><b>Zhiyuan Fang</b></a>
              <br>Applied Scientist
              <br>Amazon Alexa AI
            </td>
            <td class="person" style="text-align: center;">
              <a href="https://pratyay-banerjee.github.io/">
                <img class="centered-and-cropped" height="160px" style="border-radius:30%" src="images/pratyay.jpg">
              <br><b>Pratyay Banerjee</b></a>
              <br>Applied Scientist
              <br>Amazon Alexa AI
            </td>
          </tr>                     
        </table>

      </div>
      <hr>
      Please contact Man Luo (<a href="mailto: mluo26@asu.edu">mluo26@asu.edu</a>) or Tejas Gokhale (<a href="mailto: tgokhale@asu.edu">tgokhale@asu.edu</a>) for additional details
      <hr>
      <i> The workshop is supported by US National Science Foundation grants 1816039, 2132724 as part of Research, Education, and Outreach activities. </i>
      <hr>
      <p align='right'><i>Website maintained by Tejas Gokhale </i> </p>

  </div>
<!-- </div> -->
</body>
</html>
